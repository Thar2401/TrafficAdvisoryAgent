# Test suite runner and utilities\n\nimport unittest\nimport sys\nimport os\nimport time\nimport json\nfrom io import StringIO\n\n# Add parent directory to path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\ndef run_test_suite(test_pattern='test_*.py', verbosity=2, failfast=False):\n    \"\"\"Run the complete test suite with reporting\"\"\"\n    \n    print(\"ğŸš¦ Traffic Advisory Agent - Test Suite Runner\")\n    print(\"=\" * 50)\n    \n    # Discover and run tests\n    loader = unittest.TestLoader()\n    start_dir = os.path.dirname(os.path.abspath(__file__))\n    suite = loader.discover(start_dir, pattern=test_pattern)\n    \n    # Custom test result class for better reporting\n    class DetailedTestResult(unittest.TextTestResult):\n        def __init__(self, stream, descriptions, verbosity):\n            super().__init__(stream, descriptions, verbosity)\n            self.test_times = {}\n            self.start_time = None\n            self.total_start_time = time.time()\n        \n        def startTest(self, test):\n            super().startTest(test)\n            self.start_time = time.time()\n        \n        def stopTest(self, test):\n            super().stopTest(test)\n            if self.start_time:\n                elapsed = time.time() - self.start_time\n                self.test_times[str(test)] = elapsed\n        \n        def get_report_summary(self):\n            total_time = time.time() - self.total_start_time\n            return {\n                'total_tests': self.testsRun,\n                'failures': len(self.failures),\n                'errors': len(self.errors),\n                'skipped': len(self.skipped),\n                'total_time': total_time,\n                'success_rate': (self.testsRun - len(self.failures) - len(self.errors)) / max(self.testsRun, 1)\n            }\n    \n    # Run tests with custom result class\n    stream = StringIO()\n    runner = unittest.TextTestRunner(\n        stream=stream,\n        verbosity=verbosity,\n        resultclass=DetailedTestResult,\n        failfast=failfast\n    )\n    \n    print(f\"ğŸ” Discovering tests with pattern: {test_pattern}\")\n    print(f\"ğŸ“ Test directory: {start_dir}\")\n    print(f\"ğŸ§ª Found {suite.countTestCases()} test cases\")\n    print(\"\\n\" + \"=\"*50)\n    \n    # Run the tests\n    start_time = time.time()\n    result = runner.run(suite)\n    end_time = time.time()\n    \n    # Print the captured output\n    output = stream.getvalue()\n    print(output)\n    \n    # Generate summary report\n    summary = result.get_report_summary()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"ğŸ“Š TEST SUMMARY REPORT\")\n    print(\"=\"*50)\n    \n    print(f\"âœ… Total Tests: {summary['total_tests']}\")\n    print(f\"âŒ Failures: {summary['failures']}\")\n    print(f\"ğŸ’¥ Errors: {summary['errors']}\")\n    print(f\"â­ï¸  Skipped: {summary['skipped']}\")\n    print(f\"ğŸ¯ Success Rate: {summary['success_rate']*100:.1f}%\")\n    print(f\"â±ï¸  Total Time: {summary['total_time']:.2f} seconds\")\n    \n    # Performance analysis\n    if hasattr(result, 'test_times') and result.test_times:\n        slowest_tests = sorted(result.test_times.items(), key=lambda x: x[1], reverse=True)[:5]\n        print(\"\\nğŸŒ Slowest Tests:\")\n        for test_name, elapsed in slowest_tests:\n            print(f\"   {elapsed:.3f}s - {test_name.split('.')[-1]}\")\n    \n    # Detailed failure/error reporting\n    if result.failures:\n        print(\"\\nâŒ FAILURES:\")\n        for test, traceback in result.failures:\n            print(f\"   {test}: {traceback.split('AssertionError: ')[-1].split('\\n')[0]}\")\n    \n    if result.errors:\n        print(\"\\nğŸ’¥ ERRORS:\")\n        for test, traceback in result.errors:\n            error_msg = traceback.split('\\n')[-2] if '\\n' in traceback else traceback\n            print(f\"   {test}: {error_msg}\")\n    \n    # Return success status\n    success = len(result.failures) == 0 and len(result.errors) == 0\n    \n    if success:\n        print(\"\\nğŸ‰ All tests passed successfully!\")\n    else:\n        print(\"\\nâš ï¸  Some tests failed. Please review the output above.\")\n    \n    return success, summary\n\ndef run_specific_tests(test_modules):\n    \"\"\"Run specific test modules\"\"\"\n    \n    print(f\"ğŸ§ª Running specific test modules: {', '.join(test_modules)}\")\n    print(\"=\" * 50)\n    \n    for module_name in test_modules:\n        try:\n            print(f\"\\nğŸ”¬ Testing {module_name}...\")\n            \n            # Import and run the specific test module\n            module = __import__(module_name)\n            suite = unittest.TestLoader().loadTestsFromModule(module)\n            \n            runner = unittest.TextTestRunner(verbosity=2)\n            result = runner.run(suite)\n            \n            if result.wasSuccessful():\n                print(f\"âœ… {module_name} - All tests passed\")\n            else:\n                print(f\"âŒ {module_name} - {len(result.failures)} failures, {len(result.errors)} errors\")\n                \n        except ImportError as e:\n            print(f\"âŒ Could not import {module_name}: {e}\")\n        except Exception as e:\n            print(f\"ğŸ’¥ Error running {module_name}: {e}\")\n\ndef run_performance_tests_only():\n    \"\"\"Run only performance-related tests\"\"\"\n    \n    print(\"ğŸš€ Running Performance Tests Only\")\n    print(\"=\" * 50)\n    \n    # Load performance tests\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n    \n    try:\n        from test_integration import TestPerformanceBenchmarks, TestDataScalability\n        suite.addTest(loader.loadTestsFromTestCase(TestPerformanceBenchmarks))\n        suite.addTest(loader.loadTestsFromTestCase(TestDataScalability))\n        \n        runner = unittest.TextTestRunner(verbosity=2)\n        result = runner.run(suite)\n        \n        return result.wasSuccessful()\n        \n    except ImportError as e:\n        print(f\"âŒ Could not load performance tests: {e}\")\n        return False\n\ndef run_reliability_tests_only():\n    \"\"\"Run only reliability-related tests\"\"\"\n    \n    print(\"ğŸ›¡ï¸ Running Reliability Tests Only\")\n    print(\"=\" * 50)\n    \n    # Load reliability tests\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n    \n    try:\n        from test_integration import TestSystemReliability\n        suite.addTest(loader.loadTestsFromTestCase(TestSystemReliability))\n        \n        runner = unittest.TextTestRunner(verbosity=2)\n        result = runner.run(suite)\n        \n        return result.wasSuccessful()\n        \n    except ImportError as e:\n        print(f\"âŒ Could not load reliability tests: {e}\")\n        return False\n\ndef generate_test_report(output_file='test_report.json'):\n    \"\"\"Generate a detailed test report in JSON format\"\"\"\n    \n    print(f\"ğŸ“‹ Generating detailed test report: {output_file}\")\n    \n    # Run full test suite and collect results\n    success, summary = run_test_suite(verbosity=1)\n    \n    # Create detailed report\n    report = {\n        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n        'system_info': {\n            'python_version': sys.version,\n            'platform': sys.platform,\n            'test_directory': os.path.dirname(os.path.abspath(__file__))\n        },\n        'test_results': summary,\n        'success': success,\n        'recommendations': []\n    }\n    \n    # Add recommendations based on results\n    if summary['success_rate'] < 0.95:\n        report['recommendations'].append(\n            \"Success rate below 95% - investigate failing tests\"\n        )\n    \n    if summary['total_time'] > 30:\n        report['recommendations'].append(\n            \"Test suite taking longer than 30 seconds - consider optimization\"\n        )\n    \n    if summary['errors'] > 0:\n        report['recommendations'].append(\n            \"Errors detected - check system dependencies and configuration\"\n        )\n    \n    # Save report\n    try:\n        with open(output_file, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"âœ… Test report saved to {output_file}\")\n    except Exception as e:\n        print(f\"âŒ Could not save test report: {e}\")\n    \n    return report\n\ndef check_test_coverage():\n    \"\"\"Check test coverage across different modules\"\"\"\n    \n    print(\"ğŸ“Š Checking Test Coverage\")\n    print(\"=\" * 50)\n    \n    # Define expected test categories\n    test_categories = {\n        'Utils Tests': ['test_config', 'test_validators', 'test_data_generator'],\n        'Model Tests': ['test_traffic_predictor', 'test_route_optimizer', 'test_sustainability'],\n        'Module Tests': ['test_perception', 'test_reasoning', 'test_decision', 'test_action'],\n        'Integration Tests': ['test_system_integration', 'test_performance', 'test_reliability'],\n        'UI Tests': ['test_streamlit_app', 'test_ui_components']\n    }\n    \n    # Check which tests are available\n    test_dir = os.path.dirname(os.path.abspath(__file__))\n    available_tests = [f for f in os.listdir(test_dir) if f.startswith('test_') and f.endswith('.py')]\n    \n    print(f\"ğŸ“ Available test files: {len(available_tests)}\")\n    for test_file in available_tests:\n        print(f\"   âœ“ {test_file}\")\n    \n    print(\"\\nğŸ“‹ Test Category Coverage:\")\n    for category, expected_tests in test_categories.items():\n        found_tests = [t for t in expected_tests if any(t in f for f in available_tests)]\n        coverage = len(found_tests) / len(expected_tests) * 100\n        print(f\"   {category}: {coverage:.1f}% ({len(found_tests)}/{len(expected_tests)} tests)\")\n    \n    return available_tests\n\nif __name__ == '__main__':\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Traffic Advisory Agent Test Runner')\n    parser.add_argument('--pattern', default='test_*.py', help='Test file pattern')\n    parser.add_argument('--performance', action='store_true', help='Run only performance tests')\n    parser.add_argument('--reliability', action='store_true', help='Run only reliability tests')\n    parser.add_argument('--coverage', action='store_true', help='Check test coverage')\n    parser.add_argument('--report', action='store_true', help='Generate test report')\n    parser.add_argument('--modules', nargs='+', help='Run specific test modules')\n    parser.add_argument('--failfast', action='store_true', help='Stop on first failure')\n    \n    args = parser.parse_args()\n    \n    if args.coverage:\n        check_test_coverage()\n    elif args.performance:\n        run_performance_tests_only()\n    elif args.reliability:\n        run_reliability_tests_only()\n    elif args.report:\n        generate_test_report()\n    elif args.modules:\n        run_specific_tests(args.modules)\n    else:\n        success, summary = run_test_suite(\n            test_pattern=args.pattern,\n            failfast=args.failfast\n        )\n        \n        # Exit with appropriate code\n        sys.exit(0 if success else 1)"